# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vkOmMVf1O35lrlsnKn7LhvVG3Im5DuBk
"""

import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import os

random.seed(42)
np.random.seed(42)
torch.manual_seed(42)

from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, cohen_kappa_score
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

import optuna
from optuna.integration.mlflow import MLflowCallback
from scipy.stats import ttest_rel
import shap
import warnings

import mlflow
warnings.filterwarnings("ignore")

mlflow.set_experiment("malnutrition_prediction_optuna")

mlflow_callback = MLflowCallback(
    tracking_uri=mlflow.get_tracking_uri(),
    metric_name="balanced_accuracy"
)

def clean_columns(df):
    df.columns = [col.strip().upper().replace(" ", "_") for col in df.columns]
    return df

def make_classes(df):
    if "HAZ_MEAN" in df.columns:
        df["STUNTING_CLASS"] = df["HAZ_MEAN"].apply(lambda z: "Severe" if z < -3 else ("Moderate" if z < -2 else "Normal"))
    if "WHZ_MEAN" in df.columns:
        df["WASTING_CLASS"] = df["WHZ_MEAN"].apply(lambda z: "Severe" if z < -3 else ("Moderate" if z < -2 else "Normal"))
    if "WAZ_MEAN" in df.columns:
        df["UNDERWEIGHT_CLASS"] = df["WAZ_MEAN"].apply(lambda z: "Severe" if z < -3 else ("Moderate" if z < -2 else "Normal"))
    return df

def year_mid(val):
    try:
        s = str(val)
        if '-' in s:
            parts = s.split('-')
            return np.mean([float(x) for x in parts])
        else:
            return float(s)
    except:
        return np.nan

def add_interactions(df):
    if set(['SEX', 'AGE_START', 'YEAR_MID']).issubset(df.columns):
        df['SEX_AGE_YEAR'] = df['SEX'].astype(str) + '_' + df['AGE_START'].astype(str) + '_' + df['YEAR_MID'].astype(str)
    return df

def plot_eda(df, target_col, batch_size=25):
    print("Basic info:")
    print(df.info())
    print("\nMissingness (%):")
    print(df.isnull().mean() * 100)
    print("\nTarget distribution:")
    print(df[target_col].value_counts(dropna=False))

    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    total = len(num_cols)
    for start in range(0, total, batch_size):
        subset = num_cols[start:start+batch_size]
        df[subset].hist(bins=30, figsize=(12, 10))
        plt.suptitle(f'Numeric Distributions features {start+1} to {start+len(subset)}')
        plt.tight_layout(rect=[0, 0, 1, 0.97])
        plt.show()

    corr = df[num_cols].corr()
    plt.figure(figsize=(12, 10))
    sns.heatmap(corr, annot=False, cmap='coolwarm', linewidths=0.1)
    plt.title("Feature Correlations")
    plt.show()

def optuna_xgb(X, y, cat_cols, num_cols):
    def objective(trial):
        # Ensure no active run
        if mlflow.active_run():
            mlflow.end_run()

        with mlflow.start_run(nested=True):
            param = {
                "verbosity": 0,
                "objective": "multi:softprob" if len(np.unique(y)) > 2 else "binary:logistic",
                "num_class": len(np.unique(y)) if len(np.unique(y)) > 2 else None,
                "eval_metric": "mlogloss",
                "tree_method": "hist",
                "max_depth": trial.suggest_int("max_depth", 3, 15),
                "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
                "subsample": trial.suggest_float("subsample", 0.5, 1.0),
                "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
                "reg_alpha": trial.suggest_float("reg_alpha", 1e-8, 1.0, log=True),
                "reg_lambda": trial.suggest_float("reg_lambda", 1e-8, 1.0, log=True),
                "min_child_weight": trial.suggest_int("min_child_weight", 1, 10),
                "n_estimators": trial.suggest_int("n_estimators", 100, 500)
            }

            preprocess = ColumnTransformer([
                ("cat", Pipeline([
                    ("imp", SimpleImputer(strategy="most_frequent")),
                    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False))
                ]), cat_cols),
                ("num", Pipeline([
                    ("imp", SimpleImputer(strategy="median")),
                    ("scaler", StandardScaler())
                ]), num_cols)
            ])

            le = LabelEncoder()
            y_enc = le.fit_transform(y)
            skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
            scores = []
            for train_idx, valid_idx in skf.split(X, y_enc):
                X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
                y_train, y_valid = y_enc[train_idx], y_enc[valid_idx]

                pipe = Pipeline([
                    ("prep", preprocess),
                    ("xgb", XGBClassifier(**param, use_label_encoder=False, random_state=42))
                ])

                pipe.fit(X_train, y_train)
                preds = pipe.predict(X_valid)
                score = balanced_accuracy_score(y_valid, preds)
                scores.append(score)

            mlflow.log_metric("balanced_accuracy", np.mean(scores))
            return np.mean(scores)

    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=20, callbacks=[mlflow_callback])

    # End MLflow run explicitly to avoid conflicts
    if mlflow.active_run() is not None:
        mlflow.end_run()

    print("Optuna best params for XGBoost:", study.best_params)
    return study.best_params

def run_cv_with_methods(X, y, cat_cols, num_cols):
    results = []
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    # Encode labels once for all methods
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    for method in ['baseline', 'smote', 'cost_sensitive']:
        print(f"\nEvaluating method: {method}")
        fold_metrics = []
        for train_idx, test_idx in skf.split(X, y_encoded):
            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
            y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]

            preprocess = ColumnTransformer([
                ("cat", Pipeline([
                    ("imp", SimpleImputer(strategy="most_frequent")),
                    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False))
                ]), cat_cols),
                ("num", Pipeline([
                    ("imp", SimpleImputer(strategy="median")),
                    ("scaler", StandardScaler())
                ]), num_cols)
            ])

            if method == 'baseline':
                model = RandomForestClassifier(class_weight="balanced", random_state=42)
                pipe = Pipeline([("prep", preprocess), ("rf", model)])
                pipe.fit(X_train, y_train)
                y_pred = pipe.predict(X_test)
                y_true = y_test

            elif method == 'smote':
                model = RandomForestClassifier(class_weight="balanced", random_state=42)
                pipe = ImbPipeline([("prep", preprocess), ("smote", SMOTE(random_state=42)), ("rf", model)])
                pipe.fit(X_train, y_train)
                y_pred = pipe.predict(X_test)
                y_true = y_test

            elif method == 'cost_sensitive':
                # Calculate class weights properly
                class_counts = np.bincount(y_train)
                if len(class_counts) > 1:
                    pos_weight = class_counts[0] / class_counts[1]
                else:
                    pos_weight = 1.0

                model = XGBClassifier(scale_pos_weight=pos_weight,
                                     use_label_encoder=False,
                                     eval_metric="mlogloss",
                                     random_state=42)
                pipe = Pipeline([("prep", preprocess), ("xgb", model)])
                pipe.fit(X_train, y_train)
                y_pred = pipe.predict(X_test)
                y_true = y_test

            acc = accuracy_score(y_true, y_pred)
            bal_acc = balanced_accuracy_score(y_true, y_pred)
            f1 = f1_score(y_true, y_pred, average="macro")
            kappa = cohen_kappa_score(y_true, y_pred)
            fold_metrics.append([acc, bal_acc, f1, kappa])

        arr = np.array(fold_metrics)
        mean, std = arr.mean(axis=0), arr.std(axis=0)
        ci_95 = 1.96 * std / np.sqrt(len(fold_metrics))
        print(f"Method: {method}, Mean ± 95% CI (Accuracy, Balanced Accuracy, Macro F1, Kappa):")
        print(mean, "±", ci_95)
        results.append({
            "method": method,
            "metrics": mean,
            "ci_95": ci_95,
            "all_scores": arr
        })

    print(f"\nDEBUG: Number of methods in results: {len(results)}")
    for idx, res in enumerate(results):
        print(f"DEBUG: Index {idx}: Method = {res['method']}")

    # Fixed t-test: comparing baseline (index 0) vs SMOTE (index 1)
    if len(results) >= 2:
        t_stat, p_val = ttest_rel(results[0]['all_scores'][:, 1], results[1]['all_scores'][:, 1])
        print("\nPaired t-test for Balanced Accuracy: baseline vs SMOTE:")
        print(f"t={t_stat:.4f}, p={p_val:.4g}")
        if p_val < 0.05:
            print("Improvement is statistically significant.")
        else:
            print("No statistically significant difference.")
    else:
        print("Not enough results for t-test")

    return results, le

def explain_shap(pipe, X, y, le=None):
    model = None
    for step_name, step_obj in pipe.named_steps.items():
        if isinstance(step_obj, (RandomForestClassifier, XGBClassifier)):
            model = step_obj
            break

    if model is None:
        print("No model found in pipeline")
        return

    X_trans = pipe.named_steps['prep'].transform(X)
    feature_names = pipe.named_steps['prep'].get_feature_names_out()

    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_trans)

    # Handle multi-class case
    if isinstance(shap_values, list):
        print(f"Multi-class SHAP explanation for {len(shap_values)} classes")
        for i in range(len(shap_values)):
            class_name = le.inverse_transform([i])[0] if le else f"Class {i}"
            print(f"\nClass {class_name}:")
            importance = np.mean(np.abs(shap_values[i]), axis=0)
            top_idx = np.argsort(importance)[::-1][:5]
            for idx in top_idx:
                print(f"Feature: {feature_names[idx]}, Mean SHAP: {importance[idx]:.4f}")
    else:
        # Binary case
        importance = np.mean(np.abs(shap_values), axis=0)
        top_idx = np.argsort(importance)[::-1][:5]
        print("Top SHAP features and their mean impact:")
        for idx in top_idx:
            print(f"Feature: {feature_names[idx]}, Mean SHAP: {importance[idx]:.4f}")

def log_experiment(params, metrics, artifact_path="plots/"):
    with mlflow.start_run():
        mlflow.log_params(params)
        mlflow.log_metrics({k: v for k, v in zip(["acc", "bal_acc", "f1", "kappa"], metrics)})
        if os.path.exists(artifact_path):
            for fname in os.listdir(artifact_path):
                if fname.endswith(".png"):
                    mlflow.log_artifact(os.path.join(artifact_path, fname))

def main():
    INPUT_XLSX = "/content/drive/MyDrive/expanded-format.xlsx"
    DATA_SHEET = "Data"

    # Try to read with header=None first for inspection
    try:
        raw_df = pd.read_excel(INPUT_XLSX, sheet_name=DATA_SHEET, header=None)
    except Exception as e:
        print(f"Error reading file: {e}")
        return

    # Better header detection
    header_row = None
    tokens = {"ISOCODE", "YEAR", "SEX", "AGE_START", "AGE_END"}

    for i in range(min(10, len(raw_df))):
        row_values = [str(x).upper().strip() if pd.notnull(x) else "" for x in list(raw_df.iloc[i])]
        if any(token in row_values for token in tokens):
            header_row = i
            print(f"Detected header row: {header_row}")
            break

    if header_row is None:
        print("[WARNING] Could not detect header row, using row 0")
        header_row = 0

    try:
        df = pd.read_excel(INPUT_XLSX, sheet_name=DATA_SHEET, header=header_row)
        df = clean_columns(df)

        print(f"Data shape: {df.shape}")
        print(f"Available columns: {df.columns.tolist()}")

        if "YEAR" in df.columns:
            df["YEAR_MID"] = df["YEAR"].apply(year_mid)
        df = make_classes(df)
        df = add_interactions(df)

        context_cols = ["SEX", "AGE_START", "AGE_END", "YEAR_MID", "SEX_AGE_YEAR"]
        zmean_cols = ["HAZ_MEAN", "WHZ_MEAN", "WAZ_MEAN"]
        keep_cols = set(context_cols + zmean_cols + ["STUNTING_CLASS", "WASTING_CLASS", "UNDERWEIGHT_CLASS"])
        modeling_cols = [c for c in keep_cols if c in df.columns]

        for target in ["STUNTING_CLASS", "WASTING_CLASS", "UNDERWEIGHT_CLASS"]:
            if target not in df.columns:
                print(f"Target {target} not found. Skipping...")
                continue

            target_counts = df[target].value_counts()
            print(f"\nTarget distribution for {target}:")
            print(target_counts)

            if len(target_counts) < 2:
                print(f"Only one class in {target}. Skipping...")
                continue

            print(f"\n=== Processing target: {target} ===")

            # Remove rows with missing target
            valid_idx = df[target].notna()
            X = df.loc[valid_idx, modeling_cols].drop([target], axis=1)
            y = df.loc[valid_idx, target]

            cat_cols = [col for col in X.columns if X[col].dtype == 'O' or X[col].dtype.name == 'category']
            num_cols = [col for col in X.columns if col not in cat_cols]

            print(f"Categorical columns: {cat_cols}")
            print(f"Numerical columns: {num_cols}")
            print(f"Target classes: {y.unique()}")

            # Plot EDA
            plot_eda(df[valid_idx], target, batch_size=25)

            # Optuna tuning
            print(f"\n---Optuna tuning for XGBoost ({target})---")
            best_params = optuna_xgb(X, y, cat_cols, num_cols)

            # Cross-validation with different methods
            results, le = run_cv_with_methods(X, y, cat_cols, num_cols)

            # Final model training and SHAP explanation
            preprocess = ColumnTransformer([
                ("cat", Pipeline([
                    ("imp", SimpleImputer(strategy="most_frequent")),
                    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False))
                ]), cat_cols),
                ("num", Pipeline([
                    ("imp", SimpleImputer(strategy="median")),
                    ("scaler", StandardScaler())
                ]), num_cols)
            ])

            # Encode labels for final model training
            y_encoded = le.fit_transform(y)

            model = XGBClassifier(random_state=42, **best_params, use_label_encoder=False, eval_metric="mlogloss")
            pipe = Pipeline([("prep", preprocess), ("xgb", model)])
            pipe.fit(X, y_encoded)

            # SHAP explanation
            print(f"\n---SHAP Explanation for {target}---")
            explain_shap(pipe, X, y_encoded, le)

            # Log to MLflow
            if results:
                log_experiment(best_params, results[0]['metrics'])

    except Exception as e:
        print(f"Error in main processing: {e}")
        import traceback
        traceback.print_exc()

    print("Pipeline completed for all targets.")

if __name__ == "__main__":
    main()